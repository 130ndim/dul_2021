{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Homework10_selfsupervised.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "hse_dul",
   "language": "python",
   "display_name": "hse_dul"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8193df96d4fa4470967e41a178b9e495": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_a07d4e7f93a24e15a8e05139d8c375ec",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_fdf7b4a7883346e488500b931155a7c3",
       "IPY_MODEL_55fd058204aa4b48b1772c47506fc68c",
       "IPY_MODEL_a2085e50fdf14d7b91fc2d177ae16de4"
      ]
     }
    },
    "a07d4e7f93a24e15a8e05139d8c375ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "fdf7b4a7883346e488500b931155a7c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_4a427c3e83cd432586b0de8f269a3930",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": "Training:  14%",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_475c25ede7ef4570b184620779732aa9"
     }
    },
    "55fd058204aa4b48b1772c47506fc68c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_33d70aa5e1864f498d9c35502a0cb867",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "",
      "max": 4680,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 666,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_7bfd1536c3d645dfa40b647e58c06d22"
     }
    },
    "a2085e50fdf14d7b91fc2d177ae16de4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_81050f0728134706a2b5ad1f32f5b1d1",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 666/4680 [00:54&lt;05:20, 12.52it/s, lr=0.001, ae_loss=0.187, clf_loss=2.37, loss=0.187]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_9fc09de52a4c4f0fbdf06db502c9074c"
     }
    },
    "4a427c3e83cd432586b0de8f269a3930": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "475c25ede7ef4570b184620779732aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "33d70aa5e1864f498d9c35502a0cb867": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "7bfd1536c3d645dfa40b647e58c06d22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "81050f0728134706a2b5ad1f32f5b1d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "9fc09de52a4c4f0fbdf06db502c9074c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/130ndim/dul_2021/blob/hw10/Homework/hw10/Homework10_selfsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZT2VPDHkxZa8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1030c908-a17a-4d31-8f5d-d91d81f95455"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'dul_2021'...\n",
      "remote: Enumerating objects: 339, done.\u001B[K\n",
      "remote: Counting objects: 100% (176/176), done.\u001B[K\n",
      "remote: Compressing objects: 100% (110/110), done.\u001B[K\n",
      "remote: Total 339 (delta 100), reused 82 (delta 60), pack-reused 163\u001B[K\n",
      "Receiving objects: 100% (339/339), 55.19 MiB | 38.95 MiB/s, done.\n",
      "Resolving deltas: 100% (157/157), done.\n",
      "Processing ./dul_2021\n",
      "\u001B[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001B[0m\n",
      "Building wheels for collected packages: dul-2021\n",
      "  Building wheel for dul-2021 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=25374 sha256=ae4b77624dd1452578d8237eabc41625fd69b2c15522463a6650f9dc4e43c7e0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d7lw10dv/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
      "Successfully built dul-2021\n",
      "Installing collected packages: dul-2021\n",
      "Successfully installed dul-2021-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
    "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
    "!pip install ./dul_2021"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from dul_2021.utils.hw10_utils import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "kvPNepG2L2Hp"
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1. Context Encoder"
   ],
   "metadata": {
    "id": "86iMf3mEPaCR"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRyHjNsSDzPY"
   },
   "source": [
    "Here we will implement [context encoder](https://arxiv.org/abs/1604.07379). The context encoder structures its self-supervised learning task by inpainting masked images. For example, the figure below shows different masking shapes, such as center masking, random block masking, and segmentation masking. Note that segmentation masking (c) is not purely self-supervised since we would need to train a image segmentation model which requires labels. However, the other two masking schemes (a) and (b) and purely self-supervised.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1fhzkULYTtyMGUUF2n9dlPayJSdcY5pRv)\n",
    "\n",
    "More formally, the context encoder optimizes the following reconstruction loss:\n",
    "$$\\mathcal{L}_{rec} = \\left\\Vert \\hat{M} \\odot (x - F((1 - \\hat{M})\\odot x)) \\right\\Vert^2_2$$\n",
    "where $\\hat{M}$ is the masked region, $x$ is the image, and $F$ is the context encoder that tries to reconstruct the masked portion. In addition to the reconstruction loss, the paper introduces an adversarial loss that encourages more realistic inpaintings.\n",
    "$$L_{adv} = \\max_D \\mathbb{E}_{x\\in \\chi} [\\log(D(x)) + \\log(1 - D(F((1-\\hat{M})\\odot x)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this task we will crop central 14x14 region. You can use slightly afjusted architectures from AVB task from homework 8.\n",
    "\n",
    "**Hyperparametrs**\n",
    "\n",
    "* latent_dim = 128\n",
    "* epochs ~ 10-20\n",
    "* classifier need fewer updates than encoder-decoder part. We suggest to update it on each 10-th iteration.\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record the mse loss and adversarial losses per batch.\n",
    "3. 30 (1, 28, 28) images. Where first 10 images are random sample from testdata with removed central region. Next 10 images are reconstracted images with your trained model. Last 10 images are initial without any removal."
   ],
   "metadata": {
    "id": "uW2dsZBKEo8m"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def drop_center(img: Tensor, n: int = 14):\n",
    "    h, w = img.size(-2), img.size(-1)\n",
    "    mask = img.new_ones(1, 1, h, w)\n",
    "    hc, wc = h // 2, w // 2\n",
    "    mask[..., hc - n // 2:hc + n // 2, wc - n // 2:wc + n // 2] = 0\n",
    "    return img * mask, 1 - mask"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yGKrKhX6LLA6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ld):\n",
    "        super().__init__()\n",
    "        self._seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self._lin = nn.Linear(4 * 4 * 128, ld)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._seq(x)\n",
    "        x = x.view(-1, 4 * 4 * 128)\n",
    "        x = self._lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ld):\n",
    "        super().__init__()\n",
    "        self._lin = nn.Linear(ld, 4 * 4 * 128)\n",
    "        self._seq = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, 1, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self._lin(z)\n",
    "        x = x.view(-1, 128, 4, 4)\n",
    "        x = self._seq(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_out: int = 1):\n",
    "        super().__init__()\n",
    "        self._seq = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self._out = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._seq(x)\n",
    "        x = x.view(-1, 4 * 4 * 128)\n",
    "        x = self._out(x)\n",
    "        return x"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "U3WpuE7YLLA7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, ld: int = 64, n_drop: int = 14, clf_freq: int = 10):\n",
    "        super().__init__()\n",
    "        self._ld = ld\n",
    "        self._n_drop = n_drop\n",
    "        self._clf_freq = clf_freq\n",
    "\n",
    "        self._enc = Encoder(ld)\n",
    "        self._dec = Decoder(ld)\n",
    "        self._clf = Classifier()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._dec(self._enc(x))\n",
    "\n",
    "    def _compute_losses(self, x):\n",
    "        masked, mask = drop_center(x, self._n_drop)\n",
    "\n",
    "        x_hat = self(masked)\n",
    "\n",
    "        rec_loss = (mask * F.mse_loss(x_hat, x, reduction='none')).mean()\n",
    "        d_hat = self._clf(x_hat)\n",
    "        ae_loss = rec_loss + F.binary_cross_entropy_with_logits(d_hat, torch.ones_like(d_hat))\n",
    "\n",
    "        clf_loss = (\n",
    "                F.binary_cross_entropy_with_logits(self._clf(x), torch.ones_like(d_hat)) +\n",
    "                F.binary_cross_entropy_with_logits(self._clf(x_hat.detach()), torch.zeros_like(d_hat))\n",
    "        )\n",
    "\n",
    "        return ae_loss, clf_loss\n",
    "\n",
    "    def fit(self, train_data, n_epochs=20, lr=1e-3, bs=256, lr_gamma=0.1, n_decays=1):\n",
    "        dl = data.DataLoader(train_data, batch_size=bs, shuffle=True, drop_last=True)\n",
    "\n",
    "        opt = optim.Adam(self.parameters(), lr=lr)\n",
    "        sch = optim.lr_scheduler.StepLR(opt, max(n_epochs // (n_decays + 1), 1), lr_gamma)\n",
    "\n",
    "        ae_losses, clf_losses = [], []\n",
    "\n",
    "        bar = tqdm(total=n_epochs * len(dl), desc='Training')\n",
    "\n",
    "        res_dict = {}\n",
    "\n",
    "        for _ in range(n_epochs):\n",
    "\n",
    "            res_dict.update({'lr': sch.get_last_lr()[0]})\n",
    "            for i, batch in enumerate(dl):\n",
    "                batch, _ = batch\n",
    "\n",
    "                ae_loss, clf_loss = self._compute_losses(batch.to(self.device))\n",
    "\n",
    "                loss = ae_loss\n",
    "                if i % self._clf_freq == 0:\n",
    "                    loss += clf_loss\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                # utils.clip_grad_norm_(self._clf.parameters(), 0.1)\n",
    "                opt.step()\n",
    "\n",
    "                ae_losses.append(ae_loss.item())\n",
    "                clf_losses.append(clf_loss.item())\n",
    "\n",
    "                res_dict.update({'ae_loss': ae_losses[-1], 'clf_loss': clf_losses[-1], 'loss': loss.item()})\n",
    "\n",
    "                bar.update(1)\n",
    "                bar.set_postfix(res_dict)\n",
    "\n",
    "            sch.step()\n",
    "\n",
    "        return np.array(ae_losses), np.array(clf_losses)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self, batch: Tensor) -> Tensor:\n",
    "        batch = batch.to(self.device)\n",
    "        masked, _ = drop_center(batch, self._n_drop)\n",
    "\n",
    "        return torch.cat([masked, self(masked), batch], dim=0)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zLmNudoxLLA9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q1(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
    "    test_data: An (n_test, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array of full of mse losses\n",
    "    - a (# of training iterations, ) numpy array of full of adversarial losses\n",
    "    - a (30, 1, 28, 28) numpy array of 10 transformed images, 10 reconstructions, and 10 groundtruths\n",
    "    \"\"\"\n",
    "    model = CAE()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    ae_losses, clf_losses = model.fit(train_data, lr_gamma=0.5, n_decays=3)\n",
    "\n",
    "    test_batch = torch.stack([test_data[i][0] for i in range(10)], dim=0)\n",
    "\n",
    "    out = model.test(test_batch).cpu()\n",
    "\n",
    "    return ae_losses, clf_losses, out"
   ],
   "metadata": {
    "id": "lQnVUrB36iFT"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q1_results(q1)"
   ],
   "metadata": {
    "id": "3uQzNlV8bQMB",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339,
     "referenced_widgets": [
      "8193df96d4fa4470967e41a178b9e495",
      "a07d4e7f93a24e15a8e05139d8c375ec",
      "fdf7b4a7883346e488500b931155a7c3",
      "55fd058204aa4b48b1772c47506fc68c",
      "a2085e50fdf14d7b91fc2d177ae16de4",
      "4a427c3e83cd432586b0de8f269a3930",
      "475c25ede7ef4570b184620779732aa9",
      "33d70aa5e1864f498d9c35502a0cb867",
      "7bfd1536c3d645dfa40b647e58c06d22",
      "81050f0728134706a2b5ad1f32f5b1d1",
      "9fc09de52a4c4f0fbdf06db502c9074c"
     ]
    },
    "outputId": "6528d541-c3fe-4e58-db82-87c0fff8c295"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193df96d4fa4470967e41a178b9e495",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "Training:   0%|          | 0/4680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-43-5f2b2c180ef1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mq1_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/content/dul_2021/utils/hw10_utils.py\u001B[0m in \u001B[0;36mq1_results\u001B[0;34m(q1)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mq1_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_mnist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m     \u001B[0maeloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclfloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0mplot_ce_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maeloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclfloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-42-76393c2d2595>\u001B[0m in \u001B[0;36mq1\u001B[0;34m(train_data, test_data)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m     \u001B[0mae_losses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclf_losses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_gamma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_decays\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mtest_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtest_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-38-2bdbed7a31f7>\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_data, n_epochs, lr, bs, lr_gamma, n_decays)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mres_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'lr'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0msch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_last_lr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m                 \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 521\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 561\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    562\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 49\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     50\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0;31m# doing this so that it is consistent with all other datasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;31m# to return a PIL Image\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m         \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'L'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001B[0m in \u001B[0;36mfromarray\u001B[0;34m(obj, mode)\u001B[0m\n\u001B[1;32m   2704\u001B[0m     \u001B[0marr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__array_interface__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2705\u001B[0m     \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"shape\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2706\u001B[0;31m     \u001B[0mndim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2707\u001B[0m     \u001B[0mstrides\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"strides\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2708\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "c7OSiq3ytd85"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2. Rotations Prediction"
   ],
   "metadata": {
    "id": "W6LVLHMMjzVL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we will imlement this [paper](https://arxiv.org/abs/1803.07728). Here, model learns good representations for downstream tasks by proxy task of prediciting rotation of the original image.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
    "\n",
    "We will work with same rotations as in paper (0, 90, 180, 270). You can use architecture AVB task in hw8. Latent dim 128 and 10 epochs should be enough.\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record the loss per batch.\n",
    "2. Over the course of training, record the accuracy score for each iteration.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "TUP2DAeeIK0F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "class MNISTWrapper:\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self._dataset = dataset\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item, _ = self._dataset[idx]\n",
    "        if self._transform is not None:\n",
    "            item = self._transform(item)\n",
    "        return item\n",
    "\n",
    "\n",
    "class RandomRotateWithLabel:\n",
    "    def __init__(self):\n",
    "        self._transforms = [T.RandomRotation(90 * i) for i in range(4)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        idx = random.randint(0, 3)\n",
    "        y = torch.tensor([idx])\n",
    "        x = self._transforms[idx](x)\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class AnglePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._clf = Classifier(4)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._clf(x)\n",
    "\n",
    "    def fit(self, train_data, n_epochs=20, lr=1e-3, bs=256, lr_gamma=0.1, n_decays=1):\n",
    "        train_data = MNISTWrapper(train_data, transform=RandomRotateWithLabel())\n",
    "        dl = data.DataLoader(train_data, batch_size=bs, shuffle=True, drop_last=True)\n",
    "\n",
    "        opt = optim.Adam(self.parameters(), lr=lr)\n",
    "        sch = optim.lr_scheduler.StepLR(opt, max(n_epochs // (n_decays + 1), 1), lr_gamma)\n",
    "\n",
    "        accs, losses = [], []\n",
    "\n",
    "        bar = tqdm(total=n_epochs * len(dl), desc='Training')\n",
    "\n",
    "        res_dict = {}\n",
    "\n",
    "        for _ in range(n_epochs):\n",
    "            acc = 0\n",
    "            res_dict.update({'lr': sch.get_last_lr()[0]})\n",
    "            for i, batch in enumerate(dl):\n",
    "                x, y = batch\n",
    "                x, y = x.to(self.device), y.ravel().to(self.device)\n",
    "\n",
    "                pred = self(x)\n",
    "\n",
    "                loss = F.cross_entropy(pred, y.ravel())\n",
    "\n",
    "                acc += torch.eq(pred.argmax(-1), y).sum()\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                # utils.clip_grad_norm_(self._clf.parameters(), 0.1)\n",
    "                opt.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                res_dict.update({'loss': loss.item()})\n",
    "\n",
    "                bar.update(1)\n",
    "                bar.set_postfix(res_dict)\n",
    "\n",
    "            accs.append(acc / bs / len(dl))\n",
    "            sch.step()\n",
    "\n",
    "        return np.array(losses), np.array(accs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q2(train_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array of full of losses\n",
    "    - a (# of training epochs, ) numpy array of full of accuracy scores\n",
    "    \"\"\"\n",
    "    model = AnglePredictor()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    losses, accs = model.fit(train_data, 10, lr_gamma=0.5, n_decays=3)\n",
    "\n",
    "    return losses, accs"
   ],
   "metadata": {
    "id": "_YSch41UKy2g"
   },
   "execution_count": 93,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q2_results(q2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "VIpLkvcU39ib",
    "outputId": "ce73f4ac-53e6-4fcf-ed8b-e0b47be5cd88"
   },
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|          | 0/4680 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76faff65f5344c3b944a808b7898fece"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n",
      "torch.Size([256, 4]) torch.Size([256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_94456/3552280741.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/hse_dul/Homework/hw10/dul_2021/utils/hw10_utils.py\u001B[0m in \u001B[0;36mq2_results\u001B[0;34m(q2)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m     \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_mnist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m     \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0mplot_rt_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0macc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_94456/2325693488.py\u001B[0m in \u001B[0;36mq2\u001B[0;34m(train_data)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_94456/2808111132.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_data, n_epochs, lr, bs, lr_gamma, n_decays)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m                 \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m                 \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m                 \u001B[0;31m# utils.clip_grad_norm_(self._clf.parameters(), 0.1)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m                 \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}