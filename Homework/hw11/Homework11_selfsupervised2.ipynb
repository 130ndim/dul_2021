{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Homework11_selfsupervised2.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMYCLRaOAQTAjapKz3fdLCZ",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "hse_dul",
   "language": "python",
   "display_name": "hse_dul"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/dul_2021/blob/main/Homework/hw11/Homework11_selfsupervised2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
    "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
    "!pip install ./dul_2021"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dRz5SIFKfZD",
    "outputId": "e9270062-3e6d-4de0-d683-fad084e5c11d"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dul_2021'...\r\n",
      "remote: Enumerating objects: 361, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (190/190), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (117/117), done.\u001B[K\r\n",
      "Receiving objects:  84% (304/361), 55.31 MiB | 566.00 KiB/s\r\rceiving objects:   5% (19/361), 3.00 MiB | 1.43 MiB/sReceiving objects:   9% (33/361), 3.00 MiB | 1.43 MiB/sReceiving objects:  19% (69/361), 3.00 MiB | 1.43 MiB/sReceiving objects:  21% (76/361), 4.54 MiB | 1.74 MiB/sReceiving objects:  25% (91/361), 4.54 MiB | 1.74 MiB/sReceiving objects:  27% (101/361), 6.07 MiB | 1.95 MiB/sReceiving objects:  27% (101/361), 10.92 MiB | 2.28 MiB/sReceiving objects:  27% (101/361), 14.60 MiB | 2.16 MiB/sReceiving objects:  27% (101/361), 17.68 MiB | 1.67 MiB/sReceiving objects:  27% (101/361), 20.01 MiB | 1.32 MiB/sReceiving objects:  27% (101/361), 21.55 MiB | 1.14 MiB/sReceiving objects:  27% (101/361), 24.02 MiB | 1.07 MiB/sReceiving objects:  27% (101/361), 26.84 MiB | 1.20 MiB/sReceiving objects:  28% (102/361), 28.82 MiB | 1.30 MiB/sReceiving objects:  34% (124/361), 30.89 MiB | 1.29 MiB/sReceiving objects:  34% (124/361), 34.07 MiB | 1.43 MiB/sReceiving objects:  34% (124/361), 36.39 MiB | 1.48 MiB/sReceiving objects:  34% (124/361), 38.79 MiB | 1.36 MiB/sReceiving objects:  34% (124/361), 41.93 MiB | 1.32 MiB/sReceiving objects:  34% (124/361), 45.97 MiB | 1.63 MiB/sReceiving objects:  34% (124/361), 48.23 MiB | 1.67 MiB/sReceiving objects:  34% (124/361), 50.38 MiB | 1.35 MiB/sReceiving objects:  35% (127/361), 52.18 MiB | 1003.00 KiB/sReceiving objects:  37% (134/361), 52.18 MiB | 1003.00 KiB/sReceiving objects:  40% (146/361), 52.70 MiB | 820.00 KiB/s Receiving objects:  45% (164/361), 53.32 MiB | 749.00 KiB/sReceiving objects:  72% (260/361), 53.32 MiB | 749.00 KiB/sReceiving objects:  80% (289/361), 53.32 MiB | 749.00 KiB/sReceiving objects:  82% (297/361), 53.63 MiB | 696.00 KiB/sReceiving objects:  82% (298/361), 54.65 MiB | 539.00 KiB/sReceiving objects:  83% (300/361), 55.31 MiB | 566.00 KiB/sremote: Total 361 (delta 108), reused 93 (delta 67), pack-reused 171\u001B[K\r\n",
      "Receiving objects: 100% (361/361), 55.83 MiB | 1.31 MiB/s, done.\r\n",
      "Resolving deltas: 100% (169/169), done.\r\n",
      "Processing ./dul_2021\r\n",
      "\u001B[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001B[0m\r\n",
      "Building wheels for collected packages: dul-2021\r\n",
      "  Building wheel for dul-2021 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=26856 sha256=0ab205dab74b8dcc3d9b0603b4e06aa860ece0d9006a860f99c2e5bfd0f92782\r\n",
      "  Stored in directory: /private/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/pip-ephem-wheel-cache-fpqprvr_/wheels/f4/36/44/0f4adeafbe6490d5575db164a703203a21492463386e32170e\r\n",
      "Successfully built dul-2021\r\n",
      "Installing collected packages: dul-2021\r\n",
      "  Attempting uninstall: dul-2021\r\n",
      "    Found existing installation: dul-2021 0.1.0\r\n",
      "    Uninstalling dul-2021-0.1.0:\r\n",
      "      Successfully uninstalled dul-2021-0.1.0\r\n",
      "Successfully installed dul-2021-0.1.0\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "\n",
    "from dul_2021.utils.hw11_utils import *\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "KjY-iIy5MSZb"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1. BYOL\n",
    "\n",
    "Here we will implement [BYOL](https://arxiv.org/abs/2006.07733).\n",
    "\n",
    "* You can combine view, representation, and projection into one network. You can use same architechure as in practice. \n",
    "\n",
    "* Use BatchNorm\n",
    "\n",
    "* As predictor use few linear layers\n",
    "\n",
    "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* Ï„ = 0.99 (target update coefficient)\n",
    "* lr = 1e-4\n",
    "* num_epochs = 20\n",
    "* latent dim = 128\n",
    "\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record loss ber batch.\n",
    "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
   ],
   "metadata": {
    "id": "t7J5FgOHW6Z-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_dim=1, out_dim=128, hid_dim_full=128):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, 16, 5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 16, 3, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 4, 1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 8 * 8, hid_dim_full),\n",
    "            nn.BatchNorm1d(hid_dim_full),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hid_dim_full, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def _identity(x): return x\n",
    "\n",
    "class SSDatasetWrapper(data.Dataset):\n",
    "    def __init__(self, base, transform=_identity):\n",
    "        self._base = base\n",
    "        self._transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        orig_data, _ = self._base[idx]\n",
    "        return self._transform(orig_data), self._transform(orig_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    def __init__(self, id_=1, hd=128, od=32, gamma=0.99):\n",
    "        super().__init__()\n",
    "        self._gamma = gamma\n",
    "\n",
    "        self.student = ConvNet(id_, od, hd)\n",
    "\n",
    "        self.teacher = ConvNet(id_, od, hd)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(od, 2 * od),\n",
    "            nn.BatchNorm1d(2 * od),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2 * od, od),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def update_teacher(self):\n",
    "        for tp, sp in zip(self.teacher.parameters(), self.student.parameters()):\n",
    "            tp.data = tp * self._gamma + sp * (1 - self._gamma)\n",
    "\n",
    "    def _one_sided_loss(self, x, y):\n",
    "        pr = self.predictor(self.student(x))\n",
    "        gt = self.teacher(y)\n",
    "        return 2 * (1 - (F.normalize(pr, dim=1) * F.normalize(gt, dim=1)).sum(-1))\n",
    "\n",
    "    def _loss(self, x, y):\n",
    "        return (self._one_sided_loss(x, y) + self._one_sided_loss(y, x)).mean()\n",
    "\n",
    "    def fit(self, train_dl, n_epochs=10, lr=1e-3, wd=0.):\n",
    "        losses = []\n",
    "        opt = optim.AdamW(self.parameters(), lr=lr, weight_decay=wd)\n",
    "        with tqdm(total=n_epochs * len(train_dl)) as bar:\n",
    "            for _ in range(n_epochs):\n",
    "                for batch in train_dl:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                    loss = self._loss(x, y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    bar.update(1)\n",
    "                    bar.set_postfix(dict(loss=round(losses[-1], 5)))\n",
    "\n",
    "        return np.array(losses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.student(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q1(train_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a function that transforms batch of images into their latent representation\n",
    "    \"\"\"\n",
    "    contrast_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=32),\n",
    "        ]\n",
    "    )\n",
    "    dataset = SSDatasetWrapper(train_data, transform=contrast_transforms)\n",
    "    train_dl = data.DataLoader(dataset, batch_size=256, drop_last=True, shuffle=True)\n",
    "\n",
    "    byol = BYOL().to(DEVICE)\n",
    "\n",
    "    losses = byol.fit(train_dl)\n",
    "    byol.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode(x):\n",
    "        return byol(x).cpu()\n",
    "\n",
    "    return losses, _encode"
   ],
   "metadata": {
    "id": "Og9Fv7sV6nrO"
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
    "q1_results(q1, True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "3Ib0ne2mX80s",
    "outputId": "1acb2100-8bda-40d2-faad-852fce3f3faa"
   },
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2340 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c154a5c637743a4800a511dc172935a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/186153169.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mq1_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/hse_dul/Homework/hw11/dul_2021/utils/hw11_utils.py\u001B[0m in \u001B[0;36mq1_results\u001B[0;34m(q1, accuracy)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mq1_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'MNIST'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 137\u001B[0;31m     \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    138\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0mplot_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/2742976865.py\u001B[0m in \u001B[0;36mq1\u001B[0;34m(train_data)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0mbyol\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBYOL\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbyol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m     \u001B[0mbyol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/1601699047.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_dl, n_epochs, lr, wd)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m                     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m                     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m                     \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m                     \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2. Barlow Twins\n",
    "\n",
    "Here we will implement [barlow twins](https://arxiv.org/abs/2103.03230).\n",
    "\n",
    "* You can use same architechure as in practice. \n",
    "\n",
    "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* Î» = 0.01 \n",
    "* lr = 5e-4\n",
    "* num_epochs = 20\n",
    "* latent dim = 128\n",
    "\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record loss ber batch.\n",
    "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
   ],
   "metadata": {
    "id": "hlbaIthyMGab"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class BarlowTwins(nn.Module):\n",
    "    def __init__(self, id_=1, hd=128, od=32, lambda_=0.9):\n",
    "        super().__init__()\n",
    "        self._lambda = lambda_\n",
    "\n",
    "        self.encoder = ConvNet(id_, od, hd)\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(od, 2 * od),\n",
    "            nn.BatchNorm1d(2 * od),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2 * od, od),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def _loss(self, x, y):\n",
    "        x_norm = F.normalize(self.projector(self.encoder(x)), dim=1)\n",
    "        y_norm = F.normalize(self.projector(self.encoder(y)), dim=1)\n",
    "\n",
    "        cc = x_norm.T @ y_norm\n",
    "        N = cc.size(0)\n",
    "        loss = (cc - torch.eye(N)).pow(2).mul_(self._lambda)\n",
    "        loss[range(N), range(N)] /= self._lambda\n",
    "        return loss.sum()\n",
    "\n",
    "    def fit(self, train_dl, n_epochs=10, lr=1e-3, wd=0.):\n",
    "        losses = []\n",
    "        opt = optim.AdamW(self.parameters(), lr=lr, weight_decay=wd)\n",
    "        with tqdm(total=n_epochs * len(train_dl)) as bar:\n",
    "            for _ in range(n_epochs):\n",
    "                for batch in train_dl:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                    loss = self._loss(x, y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                    bar.update(1)\n",
    "                    bar.set_postfix(dict(loss=round(losses[-1], 5)))\n",
    "\n",
    "        return np.array(losses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q2(train_data, _):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a function that transforms batch of images into their latent representation\n",
    "    \"\"\"\n",
    "\n",
    "    contrast_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=32),\n",
    "        ]\n",
    "    )\n",
    "    dataset = SSDatasetWrapper(train_data, transform=contrast_transforms)\n",
    "    train_dl = data.DataLoader(dataset, batch_size=256, drop_last=True, shuffle=True)\n",
    "\n",
    "    barlow_twins = BarlowTwins(id_=3).to(DEVICE)\n",
    "\n",
    "    losses = barlow_twins.fit(train_dl)\n",
    "    barlow_twins.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _encode(x):\n",
    "        return barlow_twins(x).cpu()\n",
    "\n",
    "    return losses, _encode"
   ],
   "metadata": {
    "id": "pd6RrZfP75HR"
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
    "q2_results(q2, True)"
   ],
   "metadata": {
    "id": "EA1Z_s1a8_sB"
   },
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1950 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b40a6843fe2c4fb795f7a3072e3f5a89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/206886998.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/hse_dul/Homework/hw11/dul_2021/utils/hw11_utils.py\u001B[0m in \u001B[0;36mq2_results\u001B[0;34m(q2, accuracy)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'CIFAR10'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m     \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0mplot_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/524357686.py\u001B[0m in \u001B[0;36mq2\u001B[0;34m(train_data, _)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mbarlow_twins\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBarlowTwins\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid_\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     \u001B[0mlosses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbarlow_twins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m     \u001B[0mbarlow_twins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/1083044647.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, train_dl, n_epochs, lr, wd)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m                     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m                     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m                     \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m                     \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus. SwAV\n",
    "\n",
    "Here we will implement [SwAV](https://arxiv.org/abs/2006.09882v5)\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record loss ber batch.\n",
    "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
   ],
   "metadata": {
    "id": "D8UN9nr9aYGx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def b(train_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a function that transforms batch of images into their latent representation\n",
    "    \"\"\""
   ],
   "metadata": {
    "id": "1Yin_8Ebaa8P"
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q2_results(b, True)"
   ],
   "metadata": {
    "id": "S0jgIgLN8tzd"
   },
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/170498071 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a008d760d12b4484b1a4f69216483e1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/ipykernel_37769/3813442324.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/hse_dul/Homework/hw11/dul_2021/utils/hw11_utils.py\u001B[0m in \u001B[0;36mq2_results\u001B[0;34m(q2, accuracy)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mq2_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m     \u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'CIFAR10'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m     \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/hse_dul/Homework/hw11/dul_2021/utils/hw11_utils.py\u001B[0m in \u001B[0;36mget_data\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m    113\u001B[0m         \u001B[0mtest_set\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMNIST\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'./'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'CIFAR10'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m         \u001B[0mtrain_set\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCIFAR10\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'./'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m         \u001B[0mtest_set\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCIFAR10\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'./'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torchvision/datasets/cifar.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[1;32m     66\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_integrity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001B[1;32m     70\u001B[0m                                ' You can use download=True to download it')\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torchvision/datasets/cifar.py\u001B[0m in \u001B[0;36m_check_integrity\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m             \u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfentry\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfentry\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m             \u001B[0mfpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbase_folder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 136\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcheck_integrity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    137\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36mcheck_integrity\u001B[0;34m(fpath, md5)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mmd5\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mcheck_md5\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36mcheck_md5\u001B[0;34m(fpath, md5, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcheck_md5\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mmd5\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mcalculate_md5\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/hse_dul/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001B[0m in \u001B[0;36mcalculate_md5\u001B[0;34m(fpath, chunk_size)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mchunk\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mb''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m             \u001B[0mmd5\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhexdigest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}