{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Homework12_semisupervised.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyPhf+V6/HY8V6FvfB69An83",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "hse_dul",
   "language": "python",
   "display_name": "hse_dul"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GrigoryBartosh/dul_2021/blob/main/Homework/hw12/Homework12_semisupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
    "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
    "!pip install ./dul_2021"
   ],
   "metadata": {
    "id": "vEPPrGksax7-"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dul_2021'...\r\n",
      "remote: Enumerating objects: 384, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (221/221), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (147/147), done.\u001B[K\r\n",
      "remote: Total 384 (delta 124), reused 100 (delta 67), pack-reused 163\u001B[K\r\n",
      "Receiving objects: 100% (384/384), 55.90 MiB | 2.75 MiB/s, done.\r\n",
      "Resolving deltas: 100% (181/181), done.\r\n",
      "Processing ./dul_2021\r\n",
      "\u001B[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\r\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001B[0m\r\n",
      "Building wheels for collected packages: dul-2021\r\n",
      "  Building wheel for dul-2021 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=27642 sha256=73ee2c5e1cfa079c750fd48655feedb3f5e97f94a54d0b059d893a5f98c6739c\r\n",
      "  Stored in directory: /private/var/folders/zg/dd_4mghs167bb5hrrgz9w4y40000gn/T/pip-ephem-wheel-cache-7rdkt3bv/wheels/60/73/06/7bf5c79bf607444040741e4b2ffd0effe5bd0ff8607707173a\r\n",
      "Successfully built dul-2021\r\n",
      "Installing collected packages: dul-2021\r\n",
      "  Attempting uninstall: dul-2021\r\n",
      "    Found existing installation: dul-2021 0.1.0\r\n",
      "    Uninstalling dul-2021-0.1.0:\r\n",
      "      Successfully uninstalled dul-2021-0.1.0\r\n",
      "Successfully installed dul-2021-0.1.0\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from dul_2021.utils.hw12_utils import *\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from typing import TypeVar\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TorchModel = TypeVar(\"TorchModel\", bound=nn.Module)"
   ],
   "metadata": {
    "id": "VUu3B_L7azpr"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1. VAT\n",
    "\n",
    "Here we will implement [VAT](https://arxiv.org/pdf/1704.03976.pdf).\n",
    "\n",
    "* Train labeled data with standatd cross-entropy loss\n",
    "\n",
    "* Use vat regularization for both unlabeled and labeled data\n",
    "\n",
    "* You can use architecture from practice\n",
    "\n",
    "* Dataset comes as pairs `x, y`. `x` is an image from CIFAR10. `y` is a label from `[0, 10]` if datapoint is labeled and `-1` otherwise.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* Î¾= 10 \n",
    "* lr = 5e-4\n",
    "* num_epochs = 15\n",
    "\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record loss ber batch.\n",
    "2. After each epoch calculate accuracy on test data."
   ],
   "metadata": {
    "id": "qo4zHxLOv5rX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, out_dim=128, hid_dim_full=128):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1, stride=2)\n",
    "        self.conv5 = nn.Conv2d(32, 32, 1)\n",
    "        self.conv6 = nn.Conv2d(32, 4, 1)\n",
    "\n",
    "        self.conv_to_fc = 8 * 8 * 4\n",
    "        self.fc1 = nn.Linear(self.conv_to_fc, hid_dim_full)\n",
    "        self.fc2 = nn.Linear(hid_dim_full, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        x = x.view(-1, self.conv_to_fc)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return x.log_softmax(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class VATReg(nn.Module):\n",
    "    def __init__(self, xi, eps=1.0):\n",
    "        super().__init__()\n",
    "        self._xi = xi\n",
    "        self._eps = eps\n",
    "\n",
    "    def forward(self, model: TorchModel, x: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            p1 = model(x).exp()\n",
    "\n",
    "        d = torch.randn_like(x)\n",
    "        d /= d.view(d.size(0), -1).norm(dim=-1)[:, None, None, None].requires_grad_()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        log_p2 = model(x + self._xi * d)\n",
    "        d = F.kl_div(log_p2, p1, reduction='batchmean')\n",
    "        d.backward()\n",
    "\n",
    "        g = d.grad\n",
    "        g /= g.view(g.size(0), -1).norm(dim=-1)[:, None, None, None]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_p3 = model(x + g * self._eps)\n",
    "        return F.kl_div(log_p3, p1, reduction='batchmean')\n",
    "\n",
    "\n",
    "class VAT(nn.Module):\n",
    "    def __init__(self, out_dim=10, hidden_dim=128, xi=10.0, eps=1.0):\n",
    "        super().__init__()\n",
    "        self.net = Net(out_dim, hidden_dim)\n",
    "\n",
    "        self.reg = VATReg(xi, eps)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def fit(self, train_dl, test_dl, n_epochs, lr=1e-3):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        losses, accuracies = [], []\n",
    "        metric_dict = {}\n",
    "\n",
    "        with tqdm(total=(len(train_dl) + len(test_dl)) * n_epochs) as pbar:\n",
    "            for _ in range(n_epochs):\n",
    "                self.train()\n",
    "                for batch in train_dl:\n",
    "                    x, y = batch\n",
    "                    x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                    mask = y > -1\n",
    "                    loss = F.nll_loss(self.net(x[mask]), y[mask]) + self.reg(self.net, x)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    metric_dict['loss'] = loss.item()\n",
    "                    losses.append(metric_dict['loss'])\n",
    "\n",
    "                    pbar.update()\n",
    "                    pbar.set_postfix(metric_dict)\n",
    "\n",
    "                correct, total = 0, 0\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_dl:\n",
    "                        x, y = batch\n",
    "                        x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                        pred = self.net(x[mask]).argmax(-1)\n",
    "\n",
    "                        correct += (pred == y).sum().item()\n",
    "                        total += x.size(0)\n",
    "\n",
    "                        pbar.update()\n",
    "\n",
    "                    metric_dict['acc'] = correct / total\n",
    "                    accuracies.append(metric_dict['acc'])\n",
    "\n",
    "        return np.array(losses), np.array(accuracies)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q1(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 3, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a (# of training epochs, ) numpy array accuracies on each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    train_dl = data.DataLoader(train_data, batch_size=256, drop_last=True, shuffle=True)\n",
    "    test_dl = data.DataLoader(test_data, batch_size=256)\n",
    "\n",
    "    vat = VAT().to(DEVICE)\n",
    "\n",
    "    losses, accuracies = vat.fit(train_dl, test_dl)\n",
    "\n",
    "    return losses, accuracies\n"
   ],
   "metadata": {
    "id": "v3neKPxpZImJ"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q12_results(q1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "saxnnzOCzkMx",
    "outputId": "fa10bdc2-e321-4f61-b075-fb4f92ee8f2b",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/170498071 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91783dad826c4ae494840f9846bb628a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "pe5h3k7yZH_Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2. FixMatch\n",
    "\n",
    "Here we will implement [FixMatch](https://arxiv.org/abs/2001.07685).\n",
    "\n",
    "* Calculate loss on weakly augmented labeled data with standatd cross-entropy loss\n",
    "\n",
    "* Calculate loss on strongly augmented unlabeled data with standatd cross-entropy loss with pseudo-lables\n",
    "\n",
    "* Use SimCLR transformations as strong and RandomHorizontalFlip as weak\n",
    "\n",
    "* You can use architecture from practice\n",
    "\n",
    "* Dataset comes as pairs `x, y`. `x` is an image from CIFAR10. `y` is a label from `[0, 10]` if datapoint is labeled and `-1` otherwise.\n",
    "\n",
    "**Hyperparameters**\n",
    "\n",
    "* Ï = 0.7 \n",
    "* Î»_u = 10 (weight of unlabeled loss)\n",
    "* lr = 5e-4\n",
    "* num_epochs ~ 20 or more\n",
    "\n",
    "\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1. Over the course of training, record loss ber batch.\n",
    "2. After each epoch calculate accuracy on test data."
   ],
   "metadata": {
    "id": "zxgc1UFd_cpi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def q2(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 3, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a (# of training epochs, ) numpy array accuracies on each epoch\n",
    "    \"\"\""
   ],
   "metadata": {
    "id": "UZGq-KG9abw3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "q_results(q2)"
   ],
   "metadata": {
    "id": "ePG4-hS0abw_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "mzVcaBqkdd6s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus\n",
    "\n",
    "## The probabilistic model\n",
    "\n",
    "*(this is a short summary of the model presented in [\"Semi-supervised Learning with\n",
    "Deep Generative Models\"](https://arxiv.org/pdf/1406.5298.pdf))*\n",
    "\n",
    "In the semi-supervised setting, the generative model is a little more complicated than vanilla VAE. In particular, it incorporates a new variable $y$ that represents the class of a digit $x$.\n",
    "\n",
    "\\begin{align*}\n",
    "& p(x, y, z) = p(x \\mid y, z) p(z) p(y) \\\\\n",
    "& p(y) = Cat(y \\mid \\pi_0), \\pi_0 = (1/10, \\dots, 1/10) \\\\\n",
    "& p(z) = \\mathcal N(z \\mid 0, I) \\\\\n",
    "& p(x \\mid y, z) = \\prod_{i=1}^D p_i(y, z)^{x_i} (1 - p_i(y, z))^{1 - x_i}\n",
    "\\end{align*}\n",
    "\n",
    "## The first part of the objective\n",
    "\n",
    "Whenever we train a probabilistic model with partial observations, we interpret the unobserved variables as latent variables. Then we marginalize them. In this case, the loss function splits into two terms: one for observed variables (we denote the set of indices of observed labels $P$), another for unobserved.\n",
    "\n",
    "\\begin{equation}\n",
    "L(X, y) = \\sum_{i \\notin P} \\log p(x_i) + \\sum_{i \\in P} \\log p(x_i, y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Again, we can't compute the exact values of marginal likelihoods and resort to variational lower bound on likelihood. To compute lower bounds, we define the following variational approximation:\n",
    "\n",
    "\\begin{align*}\n",
    "& q(y, z \\mid x) = q(y \\mid x) q(z \\mid y, x)\\\\\n",
    "& \\\\\n",
    "& q(y \\mid x) = Cat(y \\mid \\pi(x))\\\\\n",
    "& q(z \\mid y, x) = \\mathcal N(z \\mid \\mu_\\phi(x, y), \\operatorname{diag}\\sigma^2_\\phi(y, x))\n",
    "\\end{align*}\n",
    "\n",
    "Using the variational approximation, we will obtain two lower bounds.\n",
    "\n",
    "First, the ELBO for $\\log p(x_i, y_i)$ for the observed variables (this one is similar to ELBO of VAE).\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(x, y) = \\log \\mathbb E_{p(z)} p(x, y \\mid z) \\geq \\mathbb E_{q(z \\mid y, x)} \\log \\frac{p(x, y \\mid z) p(z)}{q(z \\mid y, x)}\n",
    "\\end{equation}\n",
    "\n",
    "Second, the ELBO for $\\log p(x_i)$ for the unobserved variables.\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(x) = \\log \\mathbb E_{p(y)} \\mathbb E_{p(z \\mid y)} \\log p(x\\mid z, y)\\geq \\mathbb E_{q(y \\mid x)} \\mathbb E_{q(z \\mid y, x)} \\log \\frac{p(x, y \\mid z) p(z)}{q(z \\mid y, x) q(y \\mid x)}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the joint lower bound will be\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal L(X, y) = \\sum_{i \\in P} \\mathbb E_{q(z_i \\mid y_i, x_i)} \\log \\frac{p(x_i, y_i \\mid z_i) p(z_i)}{q(z_i \\mid y_i, x_i)} + \\sum_{i \\notin P} \\mathbb E_{q(y_i \\mid x_i)} \\mathbb E_{q(z_i \\mid y_i, x_i)} \\log \\frac{p(x_i, y_i \\mid z_i) p(z_i)}{q(z_i \\mid y_i, x_i) q(y_i \\mid x_i)}\n",
    "\\end{equation}\n",
    "\n",
    "We will use reparametrized Monte-Carlo estimates to approximate expectation w.r.t. $z$. To approximate expectaion w.r.t. the discrete variable $y$ we will try three different options."
   ],
   "metadata": {
    "id": "70Ckao5QSblC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def b(train_data, test_data):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 1, 28, 28) torchvision dataset of binary MNIST images\n",
    "    Returns\n",
    "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
    "    - a (# of training epochs, ) numpy array accuracies on each epoch\n",
    "    \"\"\""
   ],
   "metadata": {
    "id": "opRfI5beScTp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}